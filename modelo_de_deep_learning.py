# -*- coding: utf-8 -*-
"""Modelo_de_deep_learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vIo_ZMViY8X5Rqv2a6oqJZJnGKjQLnJb
"""

import pickle
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import accuracy_score
from tqdm import tqdm
import os

# config
PKL_PATH = "data/ucf101_2d.pkl"
SELECTED_CLASSES = [0, 1, 2, 3, 4]
BATCH = 16
EPOCHS = 5
LR = 1e-3
MAX_FRAMES = 300  # longitud máxima de frames (ajusta según tus datos)
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"


# cargar anotaciones con manejo de errores
print(f"Verificando archivo: {PKL_PATH}")
if not os.path.exists(PKL_PATH):
    raise FileNotFoundError(f"El archivo {PKL_PATH} no existe. Verifica la ruta.")

file_size = os.path.getsize(PKL_PATH)
print(f"Tamaño del archivo: {file_size} bytes ({file_size / (1024*1024):.2f} MB)")

if file_size == 0:
    raise ValueError(f"El archivo {PKL_PATH} está vacío (0 bytes).")

try:
    # Intentar cargar el pickle
    with open(PKL_PATH, "rb") as f:
        data = pickle.load(f)
    print("✓ Archivo pickle cargado correctamente")
except pickle.UnpicklingError as e:
    print(f"✗ Error al cargar pickle: {e}")
    print("\nPosibles soluciones:")
    print("1. El archivo está corrupto o incompleto")
    print("2. El archivo fue interrumpido durante la escritura")
    print("3. Hay un problema de compatibilidad de versiones de Python/pickle")
    print("\nIntentando cargar con pickle protocol alternativo...")

    # Intentar con diferentes protocolos
    try:
        with open(PKL_PATH, "rb") as f:
            unpickler = pickle.Unpickler(f)
            data = unpickler.load()
        print("✓ Cargado con método alternativo")
    except Exception as e2:
        print(f"✗ También falló: {e2}")
        raise Exception("No se pudo cargar el archivo pickle. Necesitas regenerar el archivo o verificar que esté completo.")
except Exception as e:
    print(f"✗ Error inesperado: {e}")
    raise

annotations = data["annotations"]
print(f"✓ Cargadas {len(annotations)} anotaciones")


# filtrar por clases
ann_filtered = [a for a in annotations if a["label"] in SELECTED_CLASSES]

# preprocesamiento
def preprocess(a):
    """
    a: entrada de un elemento de annotations
    retorna: tensor shape (MAX_FRAMES, V*2)
    """
    kp = a["keypoint"]
    # tomar solo 1 persona M=1
    kp = kp[0]

    # pasar a (T, V*2)
    kp = kp.reshape(kp.shape[0], -1).astype(np.float32)

    # normalizar
    kp = kp - kp.mean()
    std = kp.std()
    if std > 0:
        kp = kp / std

    # pad/truncate a longitud fija
    T = kp.shape[0]
    if T >= MAX_FRAMES:
        kp = kp[:MAX_FRAMES]
    else:
        pad = np.zeros((MAX_FRAMES - T, kp.shape[1]), dtype=np.float32)
        kp = np.vstack([kp, pad])

    return kp


# dataset
class UCFDataset(Dataset):
    def __init__(self, ann_list):
        self.ann = ann_list

    def __len__(self):
        return len(self.ann)

    def __getitem__(self, idx):
        a = self.ann[idx]
        x = preprocess(a)
        y = a["label"]
        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.long)


# split train/val
train_split = int(0.8 * len(ann_filtered))
train_ds = UCFDataset(ann_filtered[:train_split])
val_ds   = UCFDataset(ann_filtered[train_split:])
train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True)
val_loader   = DataLoader(val_ds, batch_size=BATCH)


# modelo
class LSTMModel(nn.Module):
    def __init__(self, input_dim, hidden, num_classes):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden, batch_first=True)
        self.fc = nn.Linear(hidden, num_classes)

    def forward(self, x):
        out, _ = self.lstm(x)
        last = out[:, -1]
        return self.fc(last)


example = preprocess(ann_filtered[0])
input_dim = example.shape[1]
num_classes = len(SELECTED_CLASSES)

model = LSTMModel(input_dim, 128, num_classes).to(DEVICE)
opt = torch.optim.Adam(model.parameters(), lr=LR)
loss_fn = nn.CrossEntropyLoss()


# entrenamiento
for epoch in range(EPOCHS):
    model.train()
    for X, y in train_loader:
        X, y = X.to(DEVICE), y.to(DEVICE)
        opt.zero_grad()
        logits = model(X)
        loss = loss_fn(logits, y)
        loss.backward()
        opt.step()

    # validación
    model.eval()
    preds, labels = [], []
    with torch.no_grad():
        for X, y in val_loader:
            X = X.to(DEVICE)
            logits = model(X)
            y_pred = logits.argmax(1).cpu().numpy()
            preds += list(y_pred)
            labels += list(y.numpy())

    acc = accuracy_score(labels, preds)
    print(f"Epoch {epoch+1}/{EPOCHS}  val_acc={acc:.4f}")